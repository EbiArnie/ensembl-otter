#!/usr/local/bin/perl

=head1 NAME

dump_pipeline - script to dump pipeline database

=head1 SYNOPSIS

dump_pipeline [options]

Options:

    --conffile, --conf=FILE             read script parameters from FILE
                                        (default: conf/Conversion.ini)
    --vega_release=NUM                  vega release number
    --release_type=STRING               type of release, ie External
    --no_feature=BOOLEAN                choose to dump *align_features or not
    --add_chr=LIST                      comma seperated list of chromsomes to add to an existing
                                        Vega db
    --ignore_chr=LIST                   comma seperated list of chromsomes to ignore
    --sql_dump_location=LOC             location of MySQLdump file
    --file_name=NAME                    name of MySQLdump file (defaults to dbname_create.sql)
 
    --pipedbname=NAME                   use pipeline database NAME
    --pipehost=HOST                     use pipeline database host HOST
    --pipeport=PORT                     use pipeline database port PORT
    --pipeuser=USER                     use pipeline database user USER
    --pipepass=PASS                     use pipeline database password PASS

    --loutredbname=NAME                 use loutre database NAME
    --loutrehost=HOST                   use loutre database host HOST
    --loutreport=PORT                   use loutre database port PORT
    --loutreuser=USER                   use loutre database username USER
    --loutrepass=PASS                   use loutre database password PASS

    --dbname=NAME                       use Vega database NAME
    --host=HOST                         use Vega database host HOST
    --port=PORT                         use Vega database port PORT
    --user=USER                         use Vega database username USER
    --pass=PASS                         use Vega database password PASS

    --logfile, --log=FILE               log to FILE (default: *STDOUT)
    --logpath=PATH                      write logfile to PATH (default: .)
    -v, --verbose                       verbose logging (default: false)
    -i, --interactive=0|1               run script interactively (default: true)
    -n, --dry_run, --dry=0|1            don't write results to database
    -h, --help, -?                      print help (this message)


=head1 DESCRIPTION

This script uses MySQLdump to read a pipeline database into a file that can be used
to create a new Vega database. The structure of all tables in the pipeline database will
be read into the file, with the exceptions of tables that are defined in the HEREDOC
at the end of the script.

The HEREDOC defines (i) tables for which all data is to be transferred, (ii) tables
which are to be completely ignored (ie not even structure), and (iii) tables for which
only some of the data is to be transferred.

Two types of constraints are used for the last of these - analysis_id (effectively hard-coded)
and seq_region_id. The latter are read from a loutre database as the non-hidden chromsomes
that have the correct seq_region_attributes for export_mode and vega_release. The wanted
values are defined in the configuration for this script, and should have been set
previously in the loutre database using another script - prepare_loutre.

Transfer of features from the dna_ and protein_align_feature tables can be prevented using
the -no_feature option

The 'add_chr' option specifies a comma separated list of chromosomes to add to an existing
database - uses just insert MySQL commands without any create table statments. In contrast
the 'chr' option manually defines a set of chromosomes to dump, ignoring the attributes set by
prepare_loutre, and uses create table statements. The 'ignore_chr' option specifies a comma
separated list of chromosomes to ignore - this does use create table statements.

=head1 LICENCE

This code is distributed under an Apache style licence:
Please see http://www.ensembl.org/code_licence.html for details

=head1 AUTHOR

Steve Trevanion <st3@sanger.ac.uk>

=head1 CONTACT

Post questions to the EnsEMBL development list ensembl-dev@ebi.ac.uk

=cut

use strict;
use warnings;
no warnings 'uninitialized';

use FindBin qw($Bin);
use vars qw($SERVERROOT);

BEGIN {
    $SERVERROOT = "$Bin/../../../..";
    unshift(@INC, "$SERVERROOT/ensembl-otter/modules");
    unshift(@INC, "$SERVERROOT/ensembl/modules");
    unshift(@INC, "modules");
    unshift(@INC, "$SERVERROOT/bioperl-live");
}

use Getopt::Long;
use Pod::Usage;
use Data::Dumper;
use Bio::EnsEMBL::Utils::ConversionSupport;
use Slice;

$| = 1;

my $support = new Bio::EnsEMBL::Utils::ConversionSupport($SERVERROOT);

# parse options
$support->parse_common_options(@_);
$support->parse_extra_options(
	'pipedumpdbname=s',
	'pipedumphost=s',
	'pipedumpport=s',
	'pipedumpuser=s',
	'vega_release=s',
	'release_type=s',
	$support->get_loutre_params,
);
$support->allowed_params(
	'pipedumpdbname',
	'pipedumphost',
	'pipedumpport',
	'pipedumpuser',
	'vega_release',
	'release_type',
	$support->get_loutre_params,
	$support->get_common_params,
);
$support->check_required_params(
	'pipedumpdbname',
	'pipedumphost',
	'pipedumpport',
	'pipedumpuser',
	'vega_release',
	'release_type',
	$support->get_loutre_params,
);
if ($support->param('help') or $support->error) {
    warn $support->error if $support->error;
    pod2usage(1);
}

$support->confirm_params;
$support->init_log;

# connect to dump of pipeline database and get adaptors
my $pddba = $support->get_database('core','pipedump');
my $pdsa  = $pddba->get_SliceAdaptor();
my $pdaa  = $pddba->get_AttributeAdaptor();
my $pddbh = $pddba->dbc->db_handle;

# connect to loutre database and get adaptors
my $ldba = $support->get_database('loutre','loutre');
my $lsa  = $ldba->get_SliceAdaptor();
my $laa  = $ldba->get_AttributeAdaptor();

#get wanted chromosomes - non-hidden ones with the correct attributes for export mode and release number
my $export_mode = $support->param('release_type');
my $release = $support->param('vega_release');
my $chr_names_wanted = &Slice::get_wanted_chromosomes($support,$laa,$lsa);

my $seq_regions = join "\n", @{$chr_names_wanted};
if (! $support->user_proceed("Data on the following assembly names will be retained in the pipeline database dump, proceed ?\n\n$seq_regions\n")) {
	exit;
}

#these are tables that are going to be cleaned up
my @target_tables = qw (analysis
						assembly
						dna
						dna_align_feature
						protein_align_feature
						simple_feature
						meta_coord
						prediction_exon
						prediction_transcript
						repeat_feature
						seq_region
						seq_region_attrib
						
					);

#store counts of no of features in each table
%table_rows;
foreach my $t (@target_tables) {
	($table_rows{$t}{'original'}) = $pddbh->selectrow_array("SELECT COUNT(*) FROM $t");
}


#####################################################
# delete any unwanted entries from meta_coord table #
#####################################################
my $target_tables = join("','",@target_tables);
my $c = $pddbh->do(qq(DELETE FROM meta_coord where table_name not in ('$target_tables')));
$support->log("Removed $c entries from meta_coord table\n");


################
# analysis ids #
################

#identify analysis_ids that are to be kept
my %ids_to_stay;
my $idsth = $pddbh->prepare(qq(
              SELECT distinct analysis_id,logic_name
              FROM analysis
    ));
$idsth->execute();
while (my ($analysis_id,$logic_name) = $idsth->fetchrow){
	#exclude _raw features for all tables
	next if ($logic_name=~/\_raw$/);
	foreach my $t qw(analysis protein_align_feature dna_align_feature) {
		push @{$ids_to_stay{$t}}, $analysis_id;
	}
	#exclude Fgenesh for prediction_transcripts
	push @{$ids_to_stay{'prediction_transcript'}}, $analysis_id unless ($logic_name eq 'Fgenesh');
	
	#exclude some logic_names from simple_feature
	next if ($logic_name=~/vertrna|est2genome|uniprot|exonerate|refseq/i);
	push @{$ids_to_stay{'simple_feature'}}, $analysis_id;
}

#delete by analysis_id
my @tables_to_clean =  qw (analysis
						   dna_align_feature
						   protein_align_feature
						   simple_feature
						   prediction_transcript
						   repeat_feature
					   );
foreach my $t (@tables_to_clean) {
	my $ids_to_keep = join("','",@{$ids_to_stay{$t}});
	my $sql = qq(DELETE from $t WHERE analysis_id not in ('$ids_to_keep'));
	if (! $support->param('dry_run')) {
		$support->log_stamped("Deleting from $t by analysis_id...\n",1);
		$table_rows{$t}{'deleted_by_anlaysis_id'} = $pddbh->execute($sql);
		$support->log_stamped("...finished deleting from $t by analysis_id...\n\n",1);
	}
}


##################
# seq_region_ids #
##################

## workout which seq_region_ids are to be kept
my $chr_to_keep = join("', '",@{$chr_names_wanted});

#chr seq_region_ids
my $sth = $pddbh->prepare(qq(
        select seq_region_id, sr.coord_system_id, cs.name
        from seq_region sr, coord_system cs
        where sr.coord_system_id = cs.coord_system_id
        and sr.name in ('$chr_name')
    ));

#contig seq_region_ids
my $sth1 =  $pddbh->prepare(qq(
        select a.cmp_seq_region_id, cs.name
        from assembly a, seq_region sr, coord_system cs
        where a.cmp_seq_region_id = sr.seq_region_id
        and sr.coord_system_id = cs.coord_system_id
        and a.asm_seq_region_id = ?
    ));

#clone seq_region_ids (links from contig_asm_seq_region_id to cmp_seq_region_id where the latter is not on the chromosome coord system
my $sth2 =  $pddbh->prepare(qq(
        select a.asm_seq_region_id, cs.name
        from assembly a, seq_region sr, coord_system cs
        where a.asm_seq_region_id = sr.seq_region_id
        and sr.coord_system_id = cs.coord_system_id
        and a.cmp_seq_region_id = ?
        and sr.coord_system_id != ?
    ));
$sth->execute;

#there might need to be some code here for other coord systems, eg zfish scaffolds ?
#if so then will need to code up deleting of these from assembly table as well (see below)

#get all relevant IDs
my %seqregions;
my @all_ids;
my @assembled_ids;
while ((my ($id, $cs_id, $csname) = $sth->fetchrow_array)) {
	push @{$seqregions{$csname}},$id;
	push @all_ids,$id;
	push @assembled_ids,$id;
	$sth1->execute($id);
	while ((my ($id, $csname) = $sth1->fetchrow_array)) {
		push @{$seqregions{$csname}},$id;
		push @all_ids,$id;
		push @assembled_ids,$id;
		$sth2->execute($id,$cs_id);
		while ((my ($id, $csname) = $sth2->fetchrow_array)) {
			push @{$seqregions{$csname}},$id;
			push @all_ids,$id;
		}
	}	
}

warn Dumper(\%seqregions);

# get coordinate systems used from meta_coord
$cssth = $pddbh->prepare(qq(
               SELECT mc.table_name, cs.name
               FROM   meta_coord mc, coord_system cs
               WHERE  mc.coord_system_id = cs.coord_system_id
               AND    mc.table_name in ('$target_tables')
       ));
$cssth->execute();

#delete from feature tables
while (my ($t, $cs_name) = $cssth->fetchrow){
	if (! $support->param('dry_run')) {
		my $ids_to_keep = join("','",@{$seq_regions{$cs_name}});
		my $sql = qq(DELETE from $t WHERE seq_region_id not in ('$ids_to_keep'));
		$support->log_stamped("Deleting from $t by seq_region_id...\n",1);
		$table_rows{$t}{'deleted_by_seq_region_id'} = $pddbh->do($sql);
		$support->log_stamped("...finished deleting from $t by seq_region_id...\n\n",1);
	}
}


##################
# seq_region etc #
##################
my $all_ids_to_stay = join("','",@all_ids);
if (! $support->param('dry_run')) {
	foreach my $t (qw(seq_region seq_region_attrib dna)) {
		$support->log_stamped("Deleting from $t by seq_region_id...\n",1);
		my $sql = qq(DELETE FROM $t where seq_region_id not in ('$all_ids_to_stay'));
		$table_rows{$t}{'deleted_by_seq_region_id'} = $pddbh->do($sql);
		$support->log_stamped("...finished deleting from $t by seq_region_id...\n\n",1);
	}
	my $assembled_ids_to_stay = join("','",@assembled_ids);
	#assembly
	$support->log_stamped("Deleting from assembly by seq_region_id...\n",1);	
	my $sql = qq(DELETE FROM assembly where asm_seq_region_id not in ('$assembled_ids_to_stay'));
	$table_rows{'assembly'}{'deleted_by_seq_region_id'} = $pddbh->do($sql);
	$support->log_stamped("...finished deleting from assembly by seq_region_id...\n\n",1);
	}
}




#########################
# seq_region_attributes #
#########################

	$gtn = $pdbh->prepare(qq(
        select distinct(cs.name)
        from seq_region_attrib sra, seq_region sr, coord_system cs
        where sra.seq_region_id = sr.seq_region_id
        and sr.coord_system_id = cs.coord_system_id
        and sr.name =  \'$chr_name\'
    ));
	$gtn->execute();
	$n  = 0;
	$na = 0;
	while (my ($cs_name) = $gtn->fetchrow){
		foreach my $sr (@{$seqregions{$cs_name}}) {
			push @{$sets{'seq_region_attrib'}{'seq_region_id'}}, $sr;
		}
	}
	
	#dna
	$gtn = $pdbh->prepare(qq(
        select distinct(cs.name)
        from dna d, seq_region sr, coord_system cs
        where d.seq_region_id = sr.seq_region_id
        and sr.coord_system_id = cs.coord_system_id
     ));
	$gtn->execute();
	while (my ($cs_name) = $gtn->fetchrow){
		foreach my $sr (@{$seqregions{$cs_name}}) {
			push @{$sets{'dna'}{'seq_region_id'}}, $sr;
		}
	}
	
	#seq_region
	foreach my $cs_name (qw(chromosome contig clone)) {	
		foreach my $sr (@{$seqregions{$cs_name}}) {
			push @{$sets{'seq_region'}{'seq_region_id'}}, $sr;
		}
	}
	
	#assembly - select on both asm_seq_region (for clones and chromosomes) and 
	# cmp_seq_region (for contigs)
	foreach my $cs_name (qw(chromosome clone)) {	
		foreach my $sr (@{$seqregions{$cs_name}}) {
			push @{$sets{'assembly'}{'asm_seq_region_id'}}, $sr;
		}
	}
	foreach my $sr (@{$seqregions{'contig'}}) {
		push @{$sets{'assembly'}{'cmp_seq_region_id'}}, $sr;
	}
	
	
#	warn Dumper(\%sets);
	
	#################################################################################
	# Read in all tables and set constraint to 's' (ie dump structure only). Then   #
	# read in the contraints in the HEREDOC, updating the above as needed ('s' =    #
	# table structure only, 'a' = table structure and all data, 'c' = constrained)  #
	#################################################################################
	
	my %constraints;
	
	#read all tables;
	map { $_ =~ s/`//g; $constraints{$_} = ['s']; } $pdbh->tables;
	
	#read details of constrained tables from HEREDOC
	my @ignored_tables;
	my $txt = &constraints;
 TABLE:
	foreach my $line (split(/\n/,$txt)){
		next if ($line =~ /^\s*$/);
		next if ($line =~ /^\#/);
		if ($line=~/^(.+)\#/){
			$line=$1;
		}
		
		my ($table,@constraints) = split(/\s+/,$line);
		
		#sanity check
		if ($table && (! exists($constraints{$table}))) {
			$support->log_warning("You have definitions for a table ($table) that is not found in the pipeline database. Skipping\n\n");
			next TABLE;
	}
		
		#if we don't want to dump align_features features for then set type from 'a' to 's'
		if ($table=~/align_feature$/ && $no_feature) {
			$support->log("\'no_feature\' option used: skipping features from $table\n");
			$constraints{$table} = ['s'];
			next TABLE;
		}
		
		#dump all data if that is how it's defined
		if ($constraints[0] eq 'a'){
			$constraints{$table} = ['a'];
			next TABLE;
		}
		
		# ignore tables that have been defined as not being part of ensembl
		if ($constraints[0] eq 'i'){
			delete $constraints{$table};
			push @ignored_tables, $table;
			next TABLE;
		}
		
		#otherwise read in the constraints
		$constraints{$table} = ['c'];	
		foreach my $cons (@constraints) {
			my ($col,$method) = split ':',$cons;
			if ($method eq 'code') {
				push @{$constraints{$table}}, $col; 
			}
			else {
				$support->log_warning("You have an unrecognised method for constraint $col\n");
			}
		}
	}
	
	#more sanity checking - ensure that the tables parsed in the script (ie code) do have an entry in the HEREDOC
	foreach my $table (keys %sets) {
		my $method = $constraints{$table}[0];
		if ($method ne 'c') {
			unless ($table=~/align_feature$/ && $no_feature) {
				$support->log_warning("You have constraints for a table ($table) generated in the code, but this is not defined in the HEREDOC in the script, please check. Skipping this table\n\n");
				delete $constraints{$table};
			}
		}
	}
	
	#don't dump structure-only tables, or other generic tables if we're adding data
	if ($support->param('add_chr') || (! $first_chr )) {
		foreach my $table (keys %constraints) {
			if ($constraints{$table}[0] eq 's') {
				delete $constraints{$table};
			}
			if (grep {$table eq $_ } qw(analysis meta_coord attrib_type coord_system meta) ) {
				delete $constraints{$table};
			}
		}
	}
	
	#do some logging the first time around:
	if ($first_chr) {
		my $sorted;
		foreach my $table (keys %constraints) {
			my $t = $constraints{$table}->[0];
			#	push @{$sorted->{$t}}, { $table => $constraints{$table} };
			$sorted->{$t}{$table} = $constraints{$table};
		}
		
		my $log = "The following tables will be ignored (ie not put into Vega):\n";
		foreach my $table (@ignored_tables) {
			$log .="\t$table\n";
		}
		$log .= "The following tables will be dumped with all their data:\n";
		foreach my $table (keys %{$sorted->{'a'}}) {
			$log .= "\t$table\n";
		}
		$log .= "The following tables will be filtered as follows:\n";
		foreach my $table (keys %{$sorted->{'c'}}) {
			my @filters =  @{$constraints{$table}};
			shift @filters;
			my $t = join ' and ', @filters;
			$log .= "\t$table: $t\n";
		}
		$log .= "The rest of the pipeline tables will be copied just with their structure (no data)\n";
		
		unless ($support->user_proceed("$log\nDo you want to proceed ?")) {
			exit;
		}
	}

	#########################
	# create mysql commands #
	#########################
	
	#initialise mysqldump statements
	my $cs;
	if($character_set) {$cs="--default-character-set=\"$character_set\"";}
	my $sei;
	if($opt_c) {$sei='--skip-extended-insert';}
	my $user   = $support->param('pipeuser');
	my $dbname = $support->param('pipedbname');
	my $host   = $support->param('pipehost');
	my $port   = $support->param('pipeport');
	my $nocreate = '';
	if ($support->param('add_chr') || (! $first_chr) ) {
		$nocreate = '-t';
	}
	my $mcom   = "mysqldump --opt --skip-lock-tables $sei $cs --single-transaction -q -u $user -P $port -h $host $dbname $nocreate";

	#create statements
	my @mysql_commands;

#	warn Dumper(\%constraints);
	
	while (my ($table,$details) = each (%constraints) ) {
		my $condition = shift @$details;
		if ($condition eq 's') {
			push @mysql_commands, "$mcom -d $table";
		}
		elsif ($condition eq 'a') {
			push @mysql_commands, "$mcom $table";
		}
		elsif ($condition eq 'c') {
			my $extra = " --where \"";
			my $i = 0;
			foreach my $cons (@$details) {
#				warn "$table--$cons";
				my @values = @{$sets{$table}{$cons}};
				my $set = join ',', @values;
				my $join = $i ? ' and' : '';
				$extra .=  "$join $cons in ($set)";
				$i++;
			}
			$extra .= " \" $table";
			push @mysql_commands, "$mcom $extra";
		}	
	}

	##################
	# do the dumping #
	##################
	
	#warn Dumper(\@mysql_commands);
	
	if (!$support->param('dry_run')) {
		foreach my $command (@mysql_commands) {
			open(MYSQL,"$command |") || die "cannot open mysql";
			my $enable;
			my $flag_disable;
			while (<MYSQL>) {
				s/(TYPE|ENGINE)=(\w+)/$1=$dbtype/;
				if (/ALTER\sTABLE\s\S+\sENABLE\sKEYS/){
					$enable=$_;
				}
				elsif (/ALTER\sTABLE\s\S+\sDISABLE\sKEYS/){
					if(!$flag_disable){
						# only write once
						$flag_disable=1;
						print OUT;
					}
				}
				else {
					print OUT;
				}
			}
			print OUT $enable if ($enable);
			close(MYSQL);
		}
		$support->log("SQL for chr $chr_name dumped to $file\n");
	}
	else {
		$support->log("\nNo SQL dumped since this is a dry run\n");
	}
	$first_chr = 0;

}
close(OUT);

$support->finish_log;

#######################################################################
# define the contraints on tables where data is to be transferred     #
# all tables not specified here will have only their structure copied #
#######################################################################

# All tables are by default dumped with just their structure. Tables for which data is also to be dumped
# are defined here, with a white space seperated list defining the columns on which to constrain mysqldump.
# A value of 'a' determines that all data in the table is to be dumped, and 'i' that the table should be
# ignored. For others, the value after the colon is usefull in that it defines how the specific values of
# the column are identified.
# - code -> specific code in the script above
# - clone / contig / chromosome -> explicitly define the coord_system to used (not actually used)

sub constraints {
	my $txt;
	$txt=<<ENDOFTEXT;
analysis              analysis_id:code      # filtering in code (without _raw)
assembly              asm_seq_region_id:code      cmp_seq_region_id:code
dna                   seq_region_id:code
dna_align_feature     seq_region_id:code          analysis_id:code
protein_align_feature seq_region_id:code          analysis_id:code
simple_feature        seq_region_id:code          analysis_id:code

meta_coord            table_name:code       # special filtering in code (no exon/transcript/gene)
prediction_exon       seq_region_id:code    # there is filtering on analysis_id, but because of link to prediction_exon then do this finish_vega_creation
prediction_transcript seq_region_id:code
repeat_feature        seq_region_id:code
seq_region            seq_region_id:code    # use all coord_systems
seq_region_attrib     seq_region_id:code    # work out which coord_systems are used 

attrib_type           a                     # should be identical to Vega
coord_system          a
meta                  a                     # should be identical to Vega
repeat_consensus      a                     # delete extra entries in finsh_vega_creation

dna_align_feature_history        i
hit_description                  i
input_id_analysis                i
input_id_seq_region              i
input_id_type_analysis           i
job                              i
job_status                       i
protein_align_feature_history    i
rule_conditions                  i
rule_goal                        i

ENDOFTEXT
	return $txt;
}


#need assembly_exception entries ?

#meta_coord table ?

#repeat consensus
