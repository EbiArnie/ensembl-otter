#!/usr/local/bin/perl

=head1 NAME

dump_pipeline - script to dump pipeline database to a .sql file

=head1 SYNOPSIS

dump_pipeline [options]

Options:

    --conffile, --conf=FILE             read script parameters from FILE
                                        (default: conf/Conversion.ini)
    --pipedbname=NAME                   use pipeline database NAME
    --pipehost=HOST                     use pipeline database host HOST
    --pipeport=PORT                     use pipeline database port PORT
    --pipeuser=USER                     use pipeline database user USER
    --pipepass=PASS                     use pipeline database password PASS

    --logfile, --log=FILE               log to FILE (default: *STDOUT)
    --logpath=PATH                      write logfile to PATH (default: .)
    -v, --verbose                       verbose logging (default: false)
    -i, --interactive=0|1               run script interactively (default: true)
    -n, --dry_run, --dry=0|1            don't write results to database
    -h, --help, -?                      print help (this message)

    -no_feature=1                       don't dump dna_ and protein_ align features
    -sql_dump_location=DIR              location of SQL dump
    -file_name=FILE                     name of SQL file

=head1 DESCRIPTION

This script uses MySQLdump to read a pipeline database (InnoDB) into a file that can be used
to create a new Vega (MyISAM) database. By default only the structure for the tables will be
read into the file. The exceptions are tables defined in the HEREDOC at the end of the script
- tables to be ignored ('i'), tables where all data is also dumped ('d'), and tables where the
features being dumped are restricted by analysis_id ('analysis_id'). Note that earlier versions
of this script (eg 1.6) attempted to restrict the transfer of all features on analysis_id and
seq_region_id - this worked for small database but fell over with human.

Transfer of features from the dna_ and protein_align_feature tables can be prevented using
the -no_feature option

To do: create database and load from .sql file

=head1 LICENCE

This code is distributed under an Apache style licence:
Please see http://www.ensembl.org/code_licence.html for details

=head1 AUTHOR

Steve Trevanion <st3@sanger.ac.uk>

=head1 CONTACT

Post questions to the EnsEMBL development list ensembl-dev@ebi.ac.uk

=cut

use strict;
use warnings;
no warnings 'uninitialized';

use FindBin qw($Bin);
use vars qw($SERVERROOT);

BEGIN {
    $SERVERROOT = "$Bin/../../../..";
    unshift(@INC, "$SERVERROOT/ensembl-otter/modules");
    unshift(@INC, "$SERVERROOT/ensembl/modules");
    unshift(@INC, "modules");
    unshift(@INC, "$SERVERROOT/bioperl-live");
}

use Getopt::Long;
use Pod::Usage;
use Data::Dumper;
use Bio::EnsEMBL::Utils::ConversionSupport;
use Slice;

$| = 1;

my $support = new Bio::EnsEMBL::Utils::ConversionSupport($SERVERROOT);

# parse options
$support->parse_common_options(@_);
$support->parse_extra_options(
	'pipedbname=s',
	'pipehost=s',
	'pipeport=s',
	'pipeuser=s',
	'no_feature=s',
	'sql_dump_location=s',
	'file_name=s',
);
$support->allowed_params(
	'pipedbname',
	'pipehost',
	'pipeport',
	'pipeuser',
	'no_feature',
	'sql_dump_location',
	'file_name',
	$support->get_common_params,
);
$support->check_required_params(
	'pipedbname',
	'pipehost',
	'pipeport',
	'pipeuser',
	'sql_dump_location',
	'dbname',
);
if ($support->param('help') or $support->error) {
    warn $support->error if $support->error;
    pod2usage(1);
}

# ask user to confirm parameters to proceed
$support->confirm_params;

# get log filehandle and print heading and parameters to logfile
$support->init_log;

#for debugging, are we dumping align_features (can be slow) or just the table structure
my $no_feature = $support->param('no_feature') || '';

#character set
my $character_set='latin1';

#database type
my $dbtype='MyISAM';

#open file to dump the sql into
my $filename = $support->param('file_name') || ($support->param('pipedbname') . '_create.sql');
my $file = $support->param('sql_dump_location') . '/' . $filename;
open(OUT,">$file") || die "cannot open $file";

# connect to pipeline database and get adaptors
my $pdba = $support->get_database('core','pipe');
my $pdbh = $pdba->dbc->db_handle;

# set up some filters - only want non *_raw_ features for  *_align_features
my %analysis_ids;
my $gtn = $pdbh->prepare(qq(
              SELECT distinct analysis_id, logic_name
              FROM analysis
    ));
$gtn->execute();
while (my ($analysis_id,$logic_name) = $gtn->fetchrow){
	#exclude for align_features
	next if ($logic_name=~/\_raw$/);
	foreach my $t qw(analysis protein_align_feature dna_align_feature) {
		push @{$analysis_ids{$t}{'analysis_id'}}, $analysis_id;
	}
#	next if ($logic_name=~/vertrna|est2genome|uniprot|exonerate|refseq/i);
#	push @{$analysis_ids{'simple_feature'}{'analysis_id'}}, $analysis_id;
}

#warn Dumper(\%analysis_ids);

#read all tables;
my %table_constraints;
map { $_ =~ s/`//g; $table_constraints{$_} = 's'; } $pdbh->tables;

#read details of constrained tables from HEREDOC
my $txt = &constraints;
TABLE:
foreach my $line (split(/\n/,$txt)){
	next if ($line =~ /^\s*$/);
	next if ($line =~ /^\#/);
	if ($line=~/^(.+)\#/){
		$line=$1;
	}
		
	my ($table,$constraint) = split(/\s+/,$line);
		
	#sanity check
	if ($table && (! exists($table_constraints{$table}))) {
		$support->log_warning("You have definitions for a table ($table) that is not found in the pipeline database. Skipping\n\n");
		next TABLE;
	}

	#skip tables to ignore
	if ($constraint eq 'i') {
		$table_constraints{$table} = 'i';
		next TABLE;
	}
		
	#if we don't want to dump align_features features for then set type to 's'
	if ($table=~/align_feature$/ && $no_feature) {
		$support->log("\'no_feature\' option used: skipping features from $table\n");
		$table_constraints{$table} = 's';
		next TABLE;
	}

	#are any constraints by analysis_id defined
	if ($analysis_ids{$table}) {
		if ($constraint eq 'analysis_id') {
			$table_constraints{$table} = $analysis_ids{$table};
		}
		else {
			$support->log_warning("Don't understand what to do with table $table\n");
		}
		next TABLE;
	}

	if ($constraint eq 'd') {
		$table_constraints{$table} = 'd';
	}
	#further sanity check
	elsif ($constraint ne 'a') {
		$support->log_warning("Constraint ($constraint) for table ($table)not understood. Skipping\n\n");
		next TABLE;
	}
}

#warn Dumper(\%table_constraints);

#Do some logging		
my $log = "The following tables will be ignored (ie not put into Vega):\n";
foreach my $table (keys %table_constraints) {
	$log .="\t$table\n" if ($table_constraints{$table} eq 'i');
}
$log .= "The following tables will be dumped with all their data:\n";
foreach my $table (keys %table_constraints) {
	$log .= "\t$table\n" if ($table_constraints{$table} eq 'd');
}

$log .= "The following tables will be dumped with some of their data:\n";
foreach my $table (keys %table_constraints) {
	if (ref($table_constraints{$table}) eq 'HASH') {
		foreach my $cons (keys %{$table_constraints{$table}}) {
			$log .= "\t$table (constrained on $cons)\n";
		}
	}
}

$log .= "The rest of the pipeline tables will be copied just with their structure (no data):\n";
foreach my $table (keys %table_constraints) {
	$log .= "\t$table\n" if ($table_constraints{$table} eq 's');
}
unless ($support->user_proceed("$log\nDo you want to proceed ?")) {
	exit;
}

#########################
# create mysql commands #
#########################

#initialise mysqldump statements
my $cs;
if(my $character_set) {$cs="--default-character-set=\"$character_set\"";}
my $sei;
if(my $opt_c) {$sei='--skip-extended-insert';}
my $user   = $support->param('pipeuser');
my $dbname = $support->param('pipedbname');
my $host   = $support->param('pipehost');
my $port   = $support->param('pipeport');

my $mcom   = "mysqldump --opt --skip-lock-tables $sei $cs --single-transaction -q -u $user -P $port -h $host $dbname";

#create statements
my @mysql_commands;

while (my ($table,$condition) = each (%table_constraints) ) {
	next if ($condition eq 'i');
	if ($condition eq 's') {
		push @mysql_commands, "$mcom -d $table";
	}
	elsif ($condition eq 'd') {
		push @mysql_commands, "$mcom $table";
	}
	elsif (ref($table_constraints{$table}) eq 'HASH') {
		foreach my $cons (keys %{$table_constraints{$table}}) {
			my $ids = join ',',@{$table_constraints{$table}{$cons}};
			my $extra = qq( --where "$cons in ($ids)");
			push @mysql_commands, "$mcom $extra $table";
		}
	}
}

##################
# do the dumping #
##################
	
#warn Dumper(\@mysql_commands);
	
if (!$support->param('dry_run')) {
	foreach my $command (@mysql_commands) {
		open(MYSQL,"$command |") || die "cannot open mysql";
		my $enable;
		my $flag_disable;
		while (<MYSQL>) {
			s/(TYPE|ENGINE)=(\w+)/$1=$dbtype/;
			if (/ALTER\sTABLE\s\S+\sENABLE\sKEYS/){
				$enable=$_;
			}
			elsif (/ALTER\sTABLE\s\S+\sDISABLE\sKEYS/){
				if(!$flag_disable){
					# only write once
					$flag_disable=1;
					print OUT;
				}
			}
			else {
				print OUT;
			}
		}
		print OUT $enable if ($enable);
		close(MYSQL);
	}
	$support->log("SQL for dumped to $file\n");
}
else {
	$support->log("\nNo SQL dumped since this is a dry run\n");
}

close(OUT);

$support->finish_log;

#######################################################################
# define the contraints on tables where data is to be transferred     #
# all tables not specified here will have only their structure copied #
#######################################################################

# All tables are by default dumped with just structure. Tables for which data is also to be dumped
# are defined here [d] as are those to be completely ignored [i]. Tables constrained on analysis_id
# are also defined

sub constraints {
	my $txt;
	$txt=<<ENDOFTEXT;
dna_align_feature_history        i
hit_description                  i
input_id_analysis                i
input_id_seq_region              i
input_id_type_analysis           i
job                              i
job_status                       i
protein_align_feature_history    i
rule_conditions                  i
rule_goal                        i

assembly                         d
seq_region                       d
seq_region_attrib                d
meta_coord                       d
coord_system                     d
dna                              d
dna_align_feature                analysis_id
protein_align_feature            analysis_id
analysis                         analysis_id
attrib_type                      d
meta                             d
prediction_exon                  d
prediction_transcript            d
repeat_consensus                 d
repeat_feature                   d
simple_feature                   d
ENDOFTEXT
	return $txt;
}
