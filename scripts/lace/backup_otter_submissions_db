#!/usr/bin/env perl

use warnings;


### backup_all_otter_submissions_databases from otterslave and archive on cbi1
### running this script as a cron job on cbi1

use strict;
use DBI;
use File::Path 'rmtree';
use Net::Netrc;

{
    umask(002);

    my $host = 'otterlive';
    my $port = '3324';
    my $mach = Net::Netrc->lookup($host);
    my $user = $mach->login;
    my $password = $mach->password;

    my $dbh = DBI->connect(
        "DBI:mysql:host=$host:port=$port",
        $user, $password, {RaiseError => 1});
    my $dbs = list_otter_databases($dbh);

    my $base_dir = '/lustre/cbi4/work1/humpub/data/mysql_backup/otter_submissions';
    my $archive_dir = "$base_dir/OTTER_SUBMISSIONS_ARCHIVE";

    cleanup_old_otter_dbs($base_dir);
    my $date_dir = date_dir();
    my $working_dir = "$base_dir/$date_dir";
    mkdir($working_dir) or die "Can't create '$working_dir' directory : $!";
    chdir($working_dir) or die "Can't chdir to '$working_dir' : $!";
  
    # As all tables are Innodb skip locking with single transaction would give
    # a consistent backup
    my $mysql_dump = "mysqldump --skip-lock-tables --single-transaction --user=$user --password=$password --host=$host --port=$port ";
    
    my $err_count = 0;
    my $backup_start = time;
    foreach my $db_name (@$dbs) {
        eval {
            print STDERR "Backing up '$db_name'\n";
            $dbh->do("use $db_name");
	    if ( $db_name =~  /^(otter|loutre)/i) {
		check_database_not_too_big($dbh);
	    }
            backup_otter_database($dbh, $mysql_dump, $db_name);
        };
        if ($@) {
            $err_count++;
            print STDERR $@;
        }
    }

    my $backup_end = time;
    printf STDERR "otter backup took %d minutes\n", ($backup_end - $backup_start) / 60;

    # tar and gzip the working directory and delete the working_dir
    # archive($archive_dir,$working_dir,$date_dir);

    # All errors reflected in the exit status
    exit $err_count;
}

sub archive {

    my ($archive_dir,$working_dir,$date_dir) = @_;
    cleanup_old_otter_dbs($archive_dir);
    my $copy_start = time;
    my $copy = "tar -cf - $working_dir | gzip -9 - > $archive_dir/$date_dir.tar.gz ";
    if (system($copy) == 0) {
        rmtree($working_dir);
    } else {
        print STDERR "ERROR: exit $? from copy command: '$copy'\n";
    }
    my $copy_end = time;
    printf STDERR "otter copy took %.0f minutes\n", ($copy_end - $copy_start) / 60;
    
}

sub check_database_not_too_big {
    my( $dbh ) = @_;
    
    my $row_count = 0;
    foreach my $table (qw{
        dna_align_feature
        protein_align_feature
        simple_feature
        })
    {
        eval{
            my $sth = $dbh->prepare(qq{ SELECT count(*) FROM $table });
            $sth->execute;
            my ($rows) = $sth->fetchrow;
            $row_count += $rows;
        };
        warn $@ if $@;
    }
    my $max_rows = 500_000;
    if ($row_count > $max_rows) {
        die "ERROR: Database has $row_count rows in feature tables: limit is $max_rows\n";
    }
}

sub backup_otter_database {
    my( $dbh, $mysql_dump, $db_name ) = @_;
    
    my $sth = $dbh->prepare("show tables");
    $sth->execute;
    my( @tables_to_backup );
    while (my ($tab) = $sth->fetchrow) {
        # The DNA table has a very large amount of data
        # in it, but we don't need to back it up.
        next if $tab eq 'dna';
        push(@tables_to_backup, $tab);
    }
    
    my $sql_file = "$db_name.sql";
    my $dump_command = "$mysql_dump $db_name @tables_to_backup > $sql_file";
    system($dump_command) == 0 or die "ERROR: exit $? running '$dump_command'\n";
}

sub list_otter_databases {
    my( $dbh ) = @_;
    
    my $sth = $dbh->prepare('show databases');
    $sth->execute;
    
    my $dbs = [];
    while (my ($db_name) = $sth->fetchrow) {
        if ($db_name =~ /^(otter|loutre|submissions|act2)/i) {
            push(@$dbs, $db_name);
        } else {
            print STDERR "Skipping database '$db_name'\n";
        }
    }
    return $dbs;
}

sub date_dir {
    my( $dbh ) = @_;
    
    my ($year, $mon, $mday, $hour, $min, $sec) = (localtime)[5,4,3,2,1,0];
    return sprintf("otter_sub_backup_%04d-%02d-%02d_%02d-%02d-%02d",
        $year + 1900, $mon + 1, $mday, $hour, $min, $sec);
}

sub cleanup_old_otter_dbs {
    my( $dir ) = @_;
    
    opendir BACKUPS, $dir or die "Can't opendir '$dir' : $!";
    my @all = sort grep /^otter_sub_backup_/, readdir BACKUPS;
    closedir BACKUPS;
    
    # Keep the most recent 12
    foreach my $i (01..12) {
        my $backup = pop(@all);
        warn "Keeping $i: $backup\n";
        last unless @all;
    }
    
    my( %seen_month );
    # Keep one from every month
    foreach my $backup (@all) {
        my ($year_month) = $backup =~ /(\d{4}-\d\d)/;
        if ($seen_month{$year_month}) {
            print STDERR "Deleteting old backup: $backup\n";
            rmtree("$dir/$backup");
        } else {
            print STDERR "Keeping: $backup\n";
            $seen_month{$year_month} = $backup;
        }
    }
}

__END__

=head1 NAME - backup_all_otter_submission_databases

=head1 AUTHOR

James Gilbert B<email> jgrg@sanger.ac.uk

modified for lutra replicated servers by 

Sindhu K. Pillai B<email> sp1@sanger.ac.uk
